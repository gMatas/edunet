{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Status\u001b[22m\u001b[39m `D:\\My Work\\Personal\\EduNet\\Project.toml`\n",
      " \u001b[90m [717857b8]\u001b[39m\u001b[37m   DSP v0.5.2\u001b[39m\n",
      " \u001b[90m [7073ff75]\u001b[39m\u001b[92m + IJulia v1.19.0\u001b[39m\n",
      " \u001b[90m [6218d12a]\u001b[39m\u001b[37m   ImageMagick v0.7.5\u001b[39m\n",
      " \u001b[90m [916415d5]\u001b[39m\u001b[37m   Images v0.18.0\u001b[39m\n",
      " \u001b[90m [37e2e46d]\u001b[39m\u001b[37m   LinearAlgebra \u001b[39m\n",
      " \u001b[90m [9a3f8284]\u001b[39m\u001b[37m   Random \u001b[39m\n",
      " \u001b[90m [10745b16]\u001b[39m\u001b[37m   Statistics \u001b[39m\n"
     ]
    }
   ],
   "source": [
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cross_entropy_prime (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# module Math\n",
    "\n",
    "# export\n",
    "# random_normal,\n",
    "# random_uniform,\n",
    "# he_normal,\n",
    "# he_uniform,\n",
    "# relu,\n",
    "# relu_prime,\n",
    "# sigmoid,\n",
    "# sigmoid_prime,\n",
    "# softmax,\n",
    "# softmax_prime,\n",
    "# square_distance,\n",
    "# square_distance_prime,\n",
    "# cross_entropy,\n",
    "# cross_entropy_prime\n",
    "\n",
    "using Random, Statistics, LinearAlgebra\n",
    "\n",
    "function random_uniform(type, dims; minval=0, maxval=nothing, rng=Random.GLOBAL_RNG)\n",
    "    if isnothing(maxval)\n",
    "        maxval = type <: AbstractFloat ? 1.0 : typemax(type)\n",
    "    else\n",
    "        maxval = convert(type, maxval)\n",
    "    end\n",
    "\n",
    "    Random.rand(rng, type, dims...) * (maxval - minval) .+ minval\n",
    "end\n",
    "\n",
    "function random_normal(type, dims; mu=0, sigma=1.0, rng=Random.GLOBAL_RNG)\n",
    "    y = Random.randn(rng, type, dims...)\n",
    "    convert(Array{type}, y * sigma .- (mean(y) - mu))\n",
    "    type.(y * sigma .- (mean(y) - mu))\n",
    "end\n",
    "\n",
    "function he_uniform(type, dims, n; minval=0, maxval=nothing, rng=Random.GLOBAL_RNG)\n",
    "    y = random_uniform(type, dims, minval=minval, maxval=maxval, rng=rng)\n",
    "    type.(y * sqrt(2 / n))\n",
    "end\n",
    "\n",
    "function he_normal(type, dims, n; mu=0, sigma=1.0, rng=Random.GLOBAL_RNG)\n",
    "    y = random_normal(type, dims, mu=mu, sigma=sigma, rng=rng)\n",
    "    type.(y * sqrt(2 / n))\n",
    "end\n",
    "\n",
    "function relu(x::Array{T, N} where {T <: AbstractFloat, N})\n",
    "    y = copy(x)\n",
    "    y[x .< 0] .= 0\n",
    "    return y\n",
    "end\n",
    "\n",
    "function relu_prime(\n",
    "    x::Array{T, N} where {T <: AbstractFloat, N},\n",
    "    gradients::Array{T, N} where {T <: AbstractFloat, N}\n",
    ")\n",
    "    indices = x .> 0\n",
    "    dy = zeros(typeof(x), size(gradients))\n",
    "    dy[indices] .= gradients[indices]\n",
    "    return dy\n",
    "end\n",
    "\n",
    "function sigmoid(x::Array{T, N} where {T <: AbstractFloat, N})\n",
    "    y = 1 ./ (1 + exp(-(x .- max(x...))))\n",
    "    return y\n",
    "end\n",
    "\n",
    "function sigmoid_prime(\n",
    "    x::Array{T, N} where {T <: AbstractFloat, N},\n",
    "    gradients::Array{T, N} where {T <: AbstractFloat, N}\n",
    ")\n",
    "    norm_x = x .- max(x...)\n",
    "    dydx = exp(-norm_x) / ((1 .+ exp(-norm_x)) .^ 2)\n",
    "    dx = gradients * dydx\n",
    "    return dx\n",
    "end\n",
    "\n",
    "function softmax(x::Vector{T} where T <: AbstractFloat)\n",
    "    exponents = exp(x .- max(x...))  # normalized exponents.\n",
    "    y = exponents ./ sum(exponents)\n",
    "    return y\n",
    "end\n",
    "\n",
    "function softmax_prime(\n",
    "    x::Vector{T} where T <: AbstractFloat,\n",
    "    gradients::Vector{T} where T <: AbstractFloat\n",
    ")\n",
    "    y = softmax(x)\n",
    "    dydx = diagm(0 => y) - (s * s')\n",
    "    dx = gradients * dydx\n",
    "    return dx\n",
    "end\n",
    "\n",
    "function square_distance(\n",
    "    x::Vector{T} where T <: AbstractFloat,\n",
    "    y::Vector{T} where T <: AbstractFloat\n",
    ")\n",
    "    if size(x) != size(y)\n",
    "        throw(DimensionMismatch(\"Input vectors dimensions does not match.\"))\n",
    "    end\n",
    "\n",
    "    e = (x - y) .^ 2\n",
    "    return e\n",
    "end\n",
    "\n",
    "function square_distance_prime(\n",
    "    x::Vector{T} where T <: AbstractFloat,\n",
    "    y::Vector{T} where T <: AbstractFloat,\n",
    "    gradients::Vector{T} where T <: AbstractFloat\n",
    ")\n",
    "    if size(x) != size(y)\n",
    "        throw(DimensionMismatch(\"Input vectors dimensions does not match.\"))\n",
    "    end\n",
    "\n",
    "    dydx = 2 .* (x - y)\n",
    "    dx = gradients * dydx'\n",
    "    return dx\n",
    "end\n",
    "\n",
    "function cross_entropy(\n",
    "    x::Vector{T} where T <: AbstractFloat,\n",
    "    y::Vector{T} where T <: AbstractFloat\n",
    ")\n",
    "    if size(x) != size(y)\n",
    "        throw(DimensionMismatch(\"Input vectors dimensions does not match.\"))\n",
    "    end\n",
    "\n",
    "    max_x = max(x...)\n",
    "    norm_logsumexp = max_x + log(sum(exp(x .- max_x)))\n",
    "    e = -dot(y, x .- norm_logsumexp)\n",
    "    return e\n",
    "end\n",
    "\n",
    "function cross_entropy_prime(\n",
    "    x::Vector{T} where T <: AbstractFloat,\n",
    "    y::Vector{T} where T <: AbstractFloat,\n",
    "    gradients::Vector{T} where T <: AbstractFloat\n",
    ")\n",
    "    if size(x) != size(y)\n",
    "        throw(DimensionMismatch(\"Input vectors dimensions does not match.\"))\n",
    "    end\n",
    "\n",
    "    dxdy = x - y\n",
    "    dx = gradients * dxdy\n",
    "    if size(dx, 1) > 1; return dx'; end\n",
    "    return dx\n",
    "end\n",
    "\n",
    "# end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convolution2d (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# module Utilities\n",
    "\n",
    "# export\n",
    "# convolution2d,\n",
    "# compute_convolution_output_size_2d,\n",
    "# CONV_FULL,\n",
    "# CONV_VALID,\n",
    "# CONV_SAME\n",
    "\n",
    "using DSP\n",
    "\n",
    "const CONV_FULL = \"full\"\n",
    "const CONV_VALID = \"valid\"\n",
    "const CONV_SAME = \"same\"\n",
    "\n",
    "function compute_convolution_output_size_2d(\n",
    "        input_size::Tuple{Integer, Integer},\n",
    "        kernel_size::Union{Integer, Tuple{Integer, Integer}},\n",
    "        strides::Union{Integer, Tuple{Integer, Integer}},\n",
    "        mode:: AbstractString = \"valid\"\n",
    "    )\n",
    "    h, w = input_size[1:2]\n",
    "    kh, kw = isa(kernel_size, Integer) ? (kernel_size, kernel_size) : kernel_size\n",
    "    sh, sw = isa(strides, Integer) ? (strides, strides) : strides\n",
    "\n",
    "    mode = lowercase(mode)\n",
    "    if mode == \"valid\"\n",
    "        output_size_h = floor(Int64, (h - kh) / sh + 1)\n",
    "        output_size_w = floor(Int64, (w - kw) / sw + 1)\n",
    "    elseif mode == \"same\"\n",
    "        output_size_h, output_size_w = input_size[1:2]\n",
    "    elseif mode == \"full\"\n",
    "        full_size_h = 2 * (kh - 1) + h\n",
    "        full_size_w = 2 * (kw - 1) + w\n",
    "        output_size_h = floor(Int64, (full_size_h - kh) / sh + 1)\n",
    "        output_size_w = floor(Int64, (full_size_w - kw) / sw + 1)\n",
    "    else\n",
    "        error(\"2-D convolution mode is not found.\")\n",
    "    end\n",
    "\n",
    "    return output_size_h, output_size_w\n",
    "end\n",
    "\n",
    "function convolution2d(src, filter; x_stride::Int=1, y_stride::Int=1, mode::String=CONV_FULL)\n",
    "    if ndims(src) != 3\n",
    "        error(\"Source array must be 3D with shape (Height x Width x Depth).\")\n",
    "    elseif ndims(filter) != 3 || size(filter, 3) != size(src, 3)\n",
    "        error(\"Filter array must be 3D with shape (Height x Width x Depth) \" *\n",
    "            \"and its depth size must be same as the source array depth.\")\n",
    "    end\n",
    "\n",
    "    mode = lowercase(mode)\n",
    "    if !(mode in [CONV_FULL, CONV_VALID, CONV_SAME])\n",
    "        error(\"No such padding mode is available. Currently available \" *\n",
    "            \"padding modes are: full | valid | same.\")\n",
    "    end\n",
    "\n",
    "    h, w, d = size(src)[1:3]\n",
    "    kernel_height, kernel_width = size(filter)[1:2]\n",
    "\n",
    "    if mode == CONV_SAME\n",
    "        x_padded_size = x_stride * (w - 1) + kernel_width\n",
    "        y_padded_size = y_stride * (h - 1) + kernel_height\n",
    "\n",
    "        x_pad_size = floor(Int, (x_padded_size - w) * .5)\n",
    "        y_pad_size = floor(Int, (y_padded_size - h) * .5)\n",
    "\n",
    "        pad = zeros(eltype(src), y_padded_size, x_padded_size, d)\n",
    "        pad[1+y_pad_size:h+y_pad_size, 1+x_pad_size:w+x_pad_size, :] = src\n",
    "        src = pad\n",
    "    end\n",
    "\n",
    "    conv_out = DSP.conv2(src[:,:,1], filter[:,:,1])\n",
    "    out = Array{eltype(conv_out), 3}(undef, size(conv_out)..., d)\n",
    "    out[:, :, 1] = conv_out\n",
    "\n",
    "    if d > 1\n",
    "        for i = 2:d\n",
    "            conv_out = DSP.conv2(src[:,:,i], filter[:,:,i])\n",
    "            out[:,:,i] = conv_out\n",
    "        end\n",
    "\n",
    "        out = sum(out, dims=3)\n",
    "    end\n",
    "\n",
    "    if mode == CONV_FULL\n",
    "        out = out[1:y_stride:end, 1:x_stride:end, :]\n",
    "    elseif mode == CONV_VALID || mode == CONV_SAME\n",
    "        out = out[\n",
    "            kernel_height:y_stride:end-kernel_height+1,\n",
    "            kernel_width:x_stride:end-kernel_width+1, :]\n",
    "    end\n",
    "\n",
    "    return out\n",
    "end\n",
    "\n",
    "# end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initialize (generic function with 13 methods)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# module Initializers\n",
    "\n",
    "# export\n",
    "# initialize,\n",
    "# Initializer,\n",
    "# HeUniform,\n",
    "# HeNormal,\n",
    "# RandomNormal,\n",
    "# RandomUniform,\n",
    "# Zeros,\n",
    "# Ones\n",
    "\n",
    "# include(\"Math.jl\")\n",
    "\n",
    "using Random\n",
    "\n",
    "# using .Math\n",
    "\n",
    "abstract type Initializer end\n",
    "\n",
    "function initialize(initializer::T where T<:Initializer); end\n",
    "\n",
    "mutable struct HeNormal <: Initializer\n",
    "    type::Type{T} where T<:Number\n",
    "    dims::Tuple{Vararg{Integer, N} where N}\n",
    "\n",
    "    HeNormal(type::Type{T} where T<:Number,\n",
    "        dims::Tuple{Vararg{Integer, N} where N}) = new(type, dims)\n",
    "end\n",
    "\n",
    "initialize(initializer::HeNormal, rng::AbstractRNG = Random.GLOBAL_RNG) =\n",
    "    he_normal(\n",
    "        initializer.type,\n",
    "        initializer.dims,\n",
    "        size(initializer.dims, 1),\n",
    "        rng=rng)\n",
    "\n",
    "mutable struct HeUniform <: Initializer\n",
    "    type::Type{T} where T<:Number\n",
    "    dims::Tuple{Vararg{Integer, N} where N}\n",
    "\n",
    "    HeUniform(type::Type{T} where T<:Number,\n",
    "        dims::Tuple{Vararg{Integer, N} where N}) = new(type, dims)\n",
    "end\n",
    "\n",
    "initialize(initializer::HeUniform, rng::AbstractRNG = Random.GLOBAL_RNG) =\n",
    "    he_uniform(\n",
    "        initializer.type,\n",
    "        initializer.dims,\n",
    "        size(initializer.dims, 1),\n",
    "        rng=rng)\n",
    "\n",
    "mutable struct RandomUniform <: Initializer\n",
    "    type::Type{T} where T<:Number\n",
    "    dims::Tuple{Vararg{Integer, N} where N}\n",
    "\n",
    "    RandomUniform(type::Type{T} where T<:Number,\n",
    "        dims::Tuple{Vararg{Integer, N} where N}) = new(type, dims)\n",
    "end\n",
    "\n",
    "initialize(initializer::RandomUniform, rng::AbstractRNG = Random.GLOBAL_RNG) =\n",
    "    random_uniform(\n",
    "        initializer.type,\n",
    "        initializer.dims,\n",
    "        rng=rng)\n",
    "\n",
    "mutable struct RandomNormal <: Initializer\n",
    "    type::Type{T} where T<:Number\n",
    "    dims::Tuple{Vararg{Integer, N} where N}\n",
    "\n",
    "    RandomNormal(type::Type{T} where T<:Number,\n",
    "        dims::Tuple{Vararg{Integer, N} where N}) = new(type, dims)\n",
    "end\n",
    "\n",
    "initialize(initializer::RandomNormal, rng::AbstractRNG = Random.GLOBAL_RNG) =\n",
    "    random_normal(\n",
    "        initializer.type,\n",
    "        initializer.dims,\n",
    "        rng=rng)\n",
    "\n",
    "mutable struct Zeros <: Initializer\n",
    "    type::Type{T} where T<:Number\n",
    "    dims::Tuple{Vararg{Integer, N} where N}\n",
    "\n",
    "    Zeros(type::Type{T} where T<:Number,\n",
    "        dims::Tuple{Vararg{Integer, N} where N}) = new(type, dims)\n",
    "end\n",
    "\n",
    "initialize(initializer::Zeros, rng::Union{AbstractRNG, Nothing} = nothing) = zeros(\n",
    "    initializer.type, initializer.dims)\n",
    "\n",
    "mutable struct Ones <: Initializer\n",
    "    type::Type{T} where T<:Number\n",
    "    dims::Tuple{Vararg{Integer, N} where N}\n",
    "\n",
    "    Ones(type::Type{T} where T<:Number,\n",
    "        dims::Tuple{Vararg{Integer, N} where N}) = new(type, dims)\n",
    "end\n",
    "\n",
    "initialize(initializer::Ones, rng::Union{AbstractRNG, Nothing} = nothing) = ones(\n",
    "    initializer.type, initializer.dims)\n",
    "\n",
    "# end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 5 methods)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# module Layers\n",
    "\n",
    "# export\n",
    "# feed!,\n",
    "# forward!,\n",
    "# backward!,\n",
    "# Input,\n",
    "# Convolution2d,\n",
    "# Reshape,\n",
    "# Relu,\n",
    "# Reshape,\n",
    "# Dense\n",
    "\n",
    "# include(\"Math.jl\")\n",
    "# include(\"Utilities.jl\")\n",
    "# include(\"Initializers.jl\")\n",
    "\n",
    "using Random\n",
    "using LinearAlgebra\n",
    "\n",
    "# using .Math\n",
    "# using .Initializers\n",
    "# import .Utilities: convolution2d\n",
    "\n",
    "abstract type LayerCache end\n",
    "\n",
    "function clear_cache!(cache::T where T<:LayerCache)\n",
    "    cache.outputs = nothing\n",
    "end\n",
    "\n",
    "macro implement_layer_cache(name::String, outputs_type::Expr)\n",
    "    struct_name = Symbol(name)\n",
    "    return quote\n",
    "        mutable struct $(struct_name) <: LayerCache\n",
    "            outputs::Union{$outputs_type, Nothing}\n",
    "            $(struct_name)() = new(nothing)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "abstract type Layer end\n",
    "\n",
    "get_inputs(layer::Layer) = layer.input_layer.cache.outputs\n",
    "\n",
    "get_outputs(layer::Layer) = layer.cache.outputs\n",
    "\n",
    "function set_outputs!(layer::Layer, outputs); layer.cache.outputs = outputs; end\n",
    "\n",
    "clear_cache!(layer::T where T<:Layer) = clear_cache!(level.cache)\n",
    "\n",
    "@implement_layer_cache(\"InputCache\", Array{T, N} where {T<:AbstractFloat, N})\n",
    "\n",
    "mutable struct Input <: Layer\n",
    "    value::AbstractArray{T, N} where {T<:AbstractFloat, N}\n",
    "    type::Type{<:AbstractFloat}\n",
    "    dims::Tuple{Vararg{Integer, N} where N}\n",
    "    cache::InputCache\n",
    "\n",
    "    function Input(type::Type{<:AbstractFloat} , dims::Tuple{Vararg{Integer, N} where N})\n",
    "        cache = InputCache()\n",
    "        cache.outputs = Array{type, length(dims)}(undef, dims)\n",
    "        new(cache.outputs, type, dims, cache)\n",
    "    end\n",
    "end\n",
    "\n",
    "function feed!(layer::Input, value::AbstractArray{T, N} where {T<:AbstractFloat, N})\n",
    "    # TODO: Finish input-feed size and type checking.\n",
    "    if layer.dims != size(value)\n",
    "        throw(DimensionMismatch(\"Given value dimensions must match that of the layer.\"))\n",
    "    end\n",
    "    if layer.type != eltype(value)\n",
    "        throw(TypeError(:feed!, Type{<:layer.type}, eltype(value)))\n",
    "    end\n",
    "    layer.value = value\n",
    "end\n",
    "\n",
    "function feed!(feed_dict::Dict{Input, AbstractArray{T, N} where {T<:AbstractFloat, N}})\n",
    "    for (layer, value) in pairs(feed_dict)\n",
    "        feed!(layer, value)\n",
    "    end\n",
    "end\n",
    "\n",
    "function feed!(feed_pairs::Pair{<:Input, <:AbstractArray{<:AbstractFloat, N} where N}...)\n",
    "    for (layer, value) in feed_pairs\n",
    "        feed!(layer, value)\n",
    "    end\n",
    "end\n",
    "\n",
    "function forward!(layer::Input)\n",
    "    outputs = layer.value\n",
    "    set_outputs!(layer, outputs)\n",
    "    return outputs\n",
    "end\n",
    "\n",
    "function backward!(layer::Input) end\n",
    "\n",
    "@implement_layer_cache(\"Convolution2dCache\", Array{<:AbstractFloat, 3})\n",
    "\n",
    "mutable struct Convolution2d <: Layer\n",
    "    input_layer::T where T<:Layer\n",
    "    weights::Array{T, 4} where T<:AbstractFloat\n",
    "    bias::Array{T, 2} where T<:AbstractFloat\n",
    "    x_stride::Integer\n",
    "    y_stride::Integer\n",
    "    mode::String\n",
    "    trainable::Bool\n",
    "    type::Type{<:AbstractFloat}\n",
    "    dims::Tuple{Vararg{Integer, N} where N}\n",
    "    cache::Convolution2dCache\n",
    "\n",
    "    function Convolution2d(\n",
    "        input_layer::T where T<:Layer,\n",
    "        filters::Integer,\n",
    "        kernel_size::Union{Integer, Tuple{Integer, Integer}};\n",
    "        strides::Union{Integer, Tuple{Integer, Integer}} = (1, 1),\n",
    "        weights_initializer::Type{T} where T<:Initializer = HeUniform,\n",
    "        bias_initializer::Type{T} where T<:Initializer = Zeros,\n",
    "        mode::String = \"valid\",\n",
    "        trainable::Bool = true,\n",
    "        rng::AbstractRNG = Random.GLOBAL_RNG\n",
    "    )\n",
    "        input_type = input_layer.type\n",
    "        input_size = input_layer.dims\n",
    "\n",
    "        # Initialize kernel and bias tensors.\n",
    "        y_kernel_size, x_kernel_size = isa(kernel_size, Integer) ?\n",
    "            (kernel_size, kernel_size) : kernel_size\n",
    "        kernel_dims = (filters, y_kernel_size, x_kernel_size, input_size[3])\n",
    "        bias_dims = (filters, 1)\n",
    "\n",
    "        weights = initialize(weights_initializer(input_type, kernel_dims), rng)\n",
    "        bias = initialize(bias_initializer(input_type, bias_dims), rng)\n",
    "\n",
    "        # Set stride parameters.\n",
    "        y_stride, x_stride = isa(strides, Integer) ? (strides, strides) : strides\n",
    "\n",
    "        # Pre-compute layer output size.\n",
    "        mode = lowercase(mode)\n",
    "        output_size_y, output_size_x = Utilities.compute_convolution_output_size_2d(\n",
    "            input_size[1:2], (y_kernel_size, x_kernel_size), (y_stride, x_stride), mode)\n",
    "        output_size = (output_size_y, output_size_x, filters)\n",
    "\n",
    "        new(input_layer, weights, bias, x_stride, y_stride, mode, trainable,\n",
    "            input_type, output_size, Convolution2dCache())\n",
    "    end\n",
    "end\n",
    "\n",
    "function forward!(layer::Convolution2d)\n",
    "    inputs = get_inputs(layer)\n",
    "    type = eltype(inputs)\n",
    "    weights = type.(layer.weights)\n",
    "    n_filters = size(weights, 1)\n",
    "\n",
    "    conv_out = convolution2d(inputs, weights[1,:,:,:],\n",
    "        x_stride=layer.x_stride, y_stride=layer.y_stride, mode=layer.mode)\n",
    "\n",
    "    if n_filters == 1; return conv_out; end\n",
    "\n",
    "    out = Array{type, 3}(undef, (size(conv_out)[1:2]..., n_filters))\n",
    "    out[:, :, 1] = conv_out\n",
    "\n",
    "    for i = 2:n_filters\n",
    "        conv_out = convolution2d(inputs, weights[1,:,:,:],\n",
    "            x_stride=layer.x_stride, y_stride=layer.y_stride, mode=layer.mode)\n",
    "        out[:, :, i] = conv_out\n",
    "    end\n",
    "\n",
    "    set_outputs!(layer, out)\n",
    "    return out\n",
    "end\n",
    "\n",
    "function backward!(layer::Convolution2d)\n",
    "end\n",
    "\n",
    "@implement_layer_cache(\"DenseCache\", Array{<: AbstractFloat, 2})\n",
    "\n",
    "mutable struct Dense <: Layer\n",
    "    input_layer::T where T<:Layer\n",
    "    weights::Matrix{<:AbstractFloat}\n",
    "    bias::Matrix{<:AbstractFloat}\n",
    "    trainable::Bool\n",
    "    type::Type{<:AbstractFloat}\n",
    "    dims::Tuple{Integer, Integer}\n",
    "    cache::DenseCache\n",
    "\n",
    "    function Dense(\n",
    "        input_layer::T where T<:Layer,\n",
    "        units::Integer;\n",
    "        weights_initializer::Type{<:Initializer} = HeUniform,\n",
    "        bias_initializer::Type{<:Initializer} = Zeros,\n",
    "        trainable::Bool = true,\n",
    "        rng::AbstractRNG = Random.GLOBAL_RNG\n",
    "    )\n",
    "        input_type = input_layer.type\n",
    "        input_size = input_layer.dims\n",
    "\n",
    "        if length(input_size) != 1 && !(length(input_size) == 2 && (input_size[1] == 1 || input_size[2] == 1))\n",
    "            throw(DimensionMismatch(\"Input layer must be 1-D.\"))\n",
    "        end\n",
    "\n",
    "        weights_dims = (units, input_size[1])\n",
    "        bias_dims = (units, 1)\n",
    "\n",
    "        weights = initialize(weights_initializer(input_type, weights_dims))\n",
    "        bias = initialize(weights_initializer(input_type, bias_dims))\n",
    "\n",
    "        output_dims = (units, 1)\n",
    "\n",
    "        new(input_layer, weights, bias, trainable, input_type, output_dims, DenseCache())\n",
    "    end\n",
    "end\n",
    "\n",
    "function forward!(layer::Dense)\n",
    "    inputs = get_inputs(layer)\n",
    "    outputs = layer.weights * inputs + layer.bias\n",
    "    set_outputs!(layer, outputs)\n",
    "    return outputs\n",
    "end\n",
    "\n",
    "function backward!(layer::Dense)\n",
    "    outputs = get_outputs(layer) \n",
    "end\n",
    "                                                                    \n",
    "@implement_layer_cache(\"ReshapeCache\", Array{<:AbstractFloat, N} where N)\n",
    "\n",
    "mutable struct Reshape <: Layer\n",
    "    input_layer::T where T<:Layer\n",
    "    type::Type{<:Real}\n",
    "    dims::Tuple{Vararg{Integer, N} where N}\n",
    "    cache::ReshapeCache\n",
    "\n",
    "    Reshape(input_layer::T where T<:Layer, dims::Tuple{Vararg{Integer, N} where N}) = new(\n",
    "        input_layer, input_layer.type, dims, ReshapeCache())\n",
    "end\n",
    "\n",
    "function forward!(layer::Reshape)\n",
    "    inputs = get_inputs(layer)\n",
    "    outputs = reshape(inputs, layer.dims)\n",
    "    set_outputs!(layer, outputs)\n",
    "    return outputs\n",
    "end\n",
    "\n",
    "function backward!(layer::Reshape)\n",
    "    outputs = reshape(layer.cache.outputs, size(layer.cache.inputs))\n",
    "    return outputs\n",
    "end\n",
    "                                                                                \n",
    "@implement_layer_cache(\"ReluCache\", Array{<: AbstractFloat, N} where N)\n",
    "\n",
    "mutable struct Relu <: Layer\n",
    "    input_layer::T where T<:Layer\n",
    "    type::Type{<:AbstractFloat}\n",
    "    dims::Tuple{Vararg{Integer, N} where N}\n",
    "    cache::ReluCache\n",
    "\n",
    "    Relu(input_layer::T where T<:Layer) = new(\n",
    "        input_layer, input_layer.type, input_layer.dims, ReluCache())\n",
    "end\n",
    "\n",
    "function forward!(layer::Relu)\n",
    "    inputs = get_inputs(layer)\n",
    "    outputs = Math.relu(inputs)\n",
    "    set_outputs!(layer, outputs)\n",
    "    return outputs\n",
    "end\n",
    "\n",
    "function backward!(layer::Relu)\n",
    "end\n",
    "\n",
    "@implement_layer_cache(\"SoftmaxCache\", Array{<: AbstractFloat, N} where N)\n",
    "                                                                                            \n",
    "mutable struct Softmax <: Layer\n",
    "    input_layer::T where T<:Layer\n",
    "    flatten_layer::Reshape\n",
    "    type::Type{<:AbstractFloat}\n",
    "    dims::Tuple{Integer, Integer}\n",
    "    cache::SoftmaxCache\n",
    "                                                                                                    \n",
    "    function Softmax(input_layer::T where T <: Layer)\n",
    "        dims = input_layer.dims\n",
    "        units = dims[1]\n",
    "        numel = prod(dims[2:end])\n",
    "        flatten_layer = Reshape(input_layer, [units, numel])\n",
    "        new(input_layer, flatten_layer, input_layer.type, dims, SoftmaxCache())\n",
    "    end\n",
    "end\n",
    "\n",
    "function forward!(layer::Softmax)\n",
    "    inputs = get_inputs(layer)\n",
    "    forward!(layer.flatten_layer)\n",
    "    flat_inputs = get_outputs(layer.flatten_layer)\n",
    "        \n",
    "end\n",
    "                                                                                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Images\n",
    "using Random\n",
    "using DSP\n",
    "\n",
    "# using Main.Utilities\n",
    "# using Main.Math\n",
    "# using Main.Layers\n",
    "# using Main.Initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_emoji_dataset_batch (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function create_emoji_dataset_batch(image_1, image_2, n; ratio=0.5, rng=Random.GLOBAL_RNG)\n",
    "    indexes = Random.randperm(rng, n)\n",
    "\n",
    "    image_1 = Float32.(permutedims(channelview(smiley_image), (2, 3, 1)))\n",
    "    image_2 = Float32.(permutedims(channelview(frown_image), (2, 3, 1)))\n",
    "\n",
    "    first_half = Int(round(n * 0.5))\n",
    "    second_half = n - first_half\n",
    "\n",
    "    data_batch_1 = repeat(reshape(image_1, (1, size(image_1)...)), first_half, 1, 1, 1)\n",
    "    data_batch_2 = repeat(reshape(image_2, (1, size(image_2)...)), second_half, 1, 1, 1)\n",
    "    data_batch = cat(data_batch_1, data_batch_2, dims=1)\n",
    "\n",
    "    labels_batch_1 = zeros(Float32, first_half, 2); labels_batch_1[:, 1] .= 1\n",
    "    labels_batch_2 = zeros(Float32, second_half, 2); labels_batch_2[:, 2] .= 1\n",
    "    labels_batch = cat(labels_batch_1, labels_batch_2, dims=1)\n",
    "\n",
    "    return data_batch[indexes, :, :, :], labels_batch[indexes, :]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAABkBAMAAACCzIhnAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAD1BMVEX/////8gAAougAAAA/SMzr7ySCAAAAAWJLR0QAiAUdSAAAAJRJREFUWMPtlM0NgDAIRtEJRB1A3cAwgUn3n0kx2L/U6hn6Th8p70IKAI0c9FhTsIANBavYUCR3nIfCg1Il6fkqNSv9drFyOXJaDCiyE49CtHM5EcVK+msUKihzcu7gNDsns0M7So2mGFDIY0MJB+E+EuFcSLoZANQr2ZziUrPyuTRQoilqlFcP6mhSMu9PtzqlYYQT/4S9Q5feResAAAAASUVORK5CYII=",
      "text/plain": [
       "20×20 Array{RGB{N0f8},2} with eltype RGB{Normed{UInt8,8}}:\n",
       " RGB{N0f8}(1.0,1.0,1.0)  RGB{N0f8}(1.0,1.0,1.0)  …  RGB{N0f8}(1.0,1.0,1.0)\n",
       " RGB{N0f8}(1.0,1.0,1.0)  RGB{N0f8}(1.0,1.0,1.0)     RGB{N0f8}(1.0,1.0,1.0)\n",
       " RGB{N0f8}(1.0,1.0,1.0)  RGB{N0f8}(1.0,1.0,1.0)     RGB{N0f8}(1.0,1.0,1.0)\n",
       " RGB{N0f8}(1.0,1.0,1.0)  RGB{N0f8}(1.0,1.0,1.0)     RGB{N0f8}(1.0,1.0,1.0)\n",
       " RGB{N0f8}(1.0,1.0,1.0)  RGB{N0f8}(1.0,1.0,1.0)     RGB{N0f8}(1.0,1.0,1.0)\n",
       " RGB{N0f8}(1.0,1.0,1.0)  RGB{N0f8}(1.0,1.0,1.0)  …  RGB{N0f8}(1.0,1.0,1.0)\n",
       " RGB{N0f8}(1.0,1.0,1.0)  RGB{N0f8}(1.0,1.0,1.0)     RGB{N0f8}(1.0,1.0,1.0)\n",
       " RGB{N0f8}(1.0,1.0,1.0)  RGB{N0f8}(1.0,1.0,1.0)     RGB{N0f8}(1.0,1.0,1.0)\n",
       " RGB{N0f8}(1.0,1.0,1.0)  RGB{N0f8}(1.0,1.0,1.0)     RGB{N0f8}(1.0,1.0,1.0)\n",
       " RGB{N0f8}(1.0,1.0,1.0)  RGB{N0f8}(1.0,1.0,1.0)     RGB{N0f8}(1.0,1.0,1.0)\n",
       " RGB{N0f8}(1.0,1.0,1.0)  RGB{N0f8}(1.0,1.0,1.0)  …  RGB{N0f8}(1.0,1.0,1.0)\n",
       " RGB{N0f8}(1.0,1.0,1.0)  RGB{N0f8}(1.0,1.0,1.0)     RGB{N0f8}(1.0,1.0,1.0)\n",
       " RGB{N0f8}(1.0,1.0,1.0)  RGB{N0f8}(1.0,1.0,1.0)     RGB{N0f8}(1.0,1.0,1.0)\n",
       " RGB{N0f8}(1.0,1.0,1.0)  RGB{N0f8}(1.0,1.0,1.0)     RGB{N0f8}(1.0,1.0,1.0)\n",
       " RGB{N0f8}(1.0,1.0,1.0)  RGB{N0f8}(1.0,1.0,1.0)     RGB{N0f8}(1.0,1.0,1.0)\n",
       " RGB{N0f8}(1.0,1.0,1.0)  RGB{N0f8}(1.0,1.0,1.0)  …  RGB{N0f8}(1.0,1.0,1.0)\n",
       " RGB{N0f8}(1.0,1.0,1.0)  RGB{N0f8}(1.0,1.0,1.0)     RGB{N0f8}(1.0,1.0,1.0)\n",
       " RGB{N0f8}(1.0,1.0,1.0)  RGB{N0f8}(1.0,1.0,1.0)     RGB{N0f8}(1.0,1.0,1.0)\n",
       " RGB{N0f8}(1.0,1.0,1.0)  RGB{N0f8}(1.0,1.0,1.0)     RGB{N0f8}(1.0,1.0,1.0)\n",
       " RGB{N0f8}(1.0,1.0,1.0)  RGB{N0f8}(1.0,1.0,1.0)     RGB{N0f8}(1.0,1.0,1.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Images\n",
    "smiley_image_filepath = raw\"D:\\My Work\\Personal\\EduNet\\god_damned_smile.bmp\"\n",
    "frown_image_filepath = raw\"D:\\My Work\\Personal\\EduNet\\god_damned_frown.bmp\"\n",
    "\n",
    "smiley_image = Images.load(smiley_image_filepath)\n",
    "frown_image = Images.load(frown_image_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float32,1}:\n",
       " 1.0\n",
       " 0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "n_batches = 1\n",
    "batch_size = 4\n",
    "\n",
    "data_batch, labels_batch = create_emoji_dataset_batch(smiley_image, frown_image, batch_size)\n",
    "\n",
    "i_sample = 1\n",
    "sample_data = data_batch[i_sample, :, :, :]\n",
    "sample_label = labels_batch[i_sample, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Input(Float32[-1.70141e38; NaN; 6.7327e-30; 0.0], Float32, (4, 1), InputCache(Float32[-1.70141e38; NaN; 6.7327e-30; 0.0]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = Input(Float32, (4, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dense(Input(Float32[-1.70141e38; NaN; 6.7327e-30; 0.0], Float32, (4, 1), InputCache(Float32[-1.70141e38; NaN; 6.7327e-30; 0.0])), Float32[0.879798 0.618542 0.662688 0.76449; 0.132764 0.78727 0.882771 0.293386], Float32[0.17803; 0.403322], true, Float32, (2, 1), DenseCache(nothing))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_1 = Dense(input_data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×1 Array{Float32,2}:\n",
       " 1.0\n",
       " 0.5\n",
       " 1.0\n",
       " 1.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = [1.0 0.5 1.0 1.5]\n",
    "value = Float32.(permutedims(value, [2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×1 Array{Float32,2}:\n",
       " 1.0\n",
       " 0.5\n",
       " 1.0\n",
       " 1.5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed!(input_data, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×1 Array{Float32,2}:\n",
       " 1.0\n",
       " 0.5\n",
       " 1.0\n",
       " 1.5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward!(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Input(Float32[1.0; 0.5; 1.0; 1.5], Float32, (4, 1), InputCache(Float32[1.0; 0.5; 1.0; 1.5]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×1 Array{Float32,2}:\n",
       " 3.1765223\n",
       " 2.25257  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward!(dense_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dense(Input(Float32[1.0; 0.5; 1.0; 1.5], Float32, (4, 1), InputCache(Float32[1.0; 0.5; 1.0; 1.5])), Float32[0.879798 0.618542 0.662688 0.76449; 0.132764 0.78727 0.882771 0.293386], Float32[0.17803; 0.403322], true, Float32, (2, 1), DenseCache(Float32[3.17652; 2.25257]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
